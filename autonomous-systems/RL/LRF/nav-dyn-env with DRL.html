<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Learning to Navigate Through Complex Dynamic Environment With Modular Deep Reinforcement Learning</title>
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="../../../favicon.svg">
        
        
        <link rel="shortcut icon" href="../../../favicon.png">
        
        <link rel="stylesheet" href="../../../css/variables.css">
        <link rel="stylesheet" href="../../../css/general.css">
        <link rel="stylesheet" href="../../../css/chrome.css">
        
        <link rel="stylesheet" href="../../../css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="../../../FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="../../../fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../../highlight.css">
        <link rel="stylesheet" href="../../../tomorrow-night.css">
        <link rel="stylesheet" href="../../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../../../meta-learning/index.html"><strong aria-hidden="true">1.</strong> Meta-Learning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../meta-learning/MAML.html"><strong aria-hidden="true">1.1.</strong> Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a></li><li class="chapter-item expanded "><a href="../../../meta-learning/MAML++.html"><strong aria-hidden="true">1.2.</strong> HOW TO TRAIN YOUR MAML</a></li><li class="chapter-item expanded "><a href="../../../meta-learning/ANIL.html"><strong aria-hidden="true">1.3.</strong> RAPID LEARNING OR FEATURE REUSE? TOWARDS UNDERSTANDING THE EFFECTIVENESS OF MAML</a></li><li class="chapter-item expanded "><a href="../../../meta-learning/FO-MAML.html"><strong aria-hidden="true">1.4.</strong> On First-Order Meta-Learning Algorithms</a></li><li class="chapter-item expanded "><a href="../../../meta-learning/HF-MAML.html"><strong aria-hidden="true">1.5.</strong> On the Convergence Theory of Gradient-Based Model-Agnostic Meta-Learning Algorithms</a></li><li class="chapter-item expanded "><a href="../../../meta-learning/iMAML.html"><strong aria-hidden="true">1.6.</strong> Meta-Learning with Implicit Gradients</a></li></ol></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/index.html"><strong aria-hidden="true">2.</strong> Autonomous Systems</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../autonomous-systems/planning&decision-making.html"><strong aria-hidden="true">2.1.</strong> [review]Planning and Decision-Making for Autonomous Vehicles</a></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/motion-planning-review.html"><strong aria-hidden="true">2.2.</strong> [review]A Review of Mobile Robot Motion Planning Methods</a></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/RL/index.html"><strong aria-hidden="true">2.3.</strong> RL</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../autonomous-systems/RL/A3C.html"><strong aria-hidden="true">2.3.1.</strong> Asynchronous Methods for Deep Reinforcement Learning (A3C)</a></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/RL/nav-in-complex-env.html"><strong aria-hidden="true">2.3.2.</strong> LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS</a></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/RL/LRF/curiosity-driven.html"><strong aria-hidden="true">2.3.3.</strong> Curiosity-driven Exploration by Self-supervised Prediction</a></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/RL/LRF/end-to-end nav strategy with DRL.html"><strong aria-hidden="true">2.3.4.</strong> End-to-End Navigation Strategy With Deep Reinforcement Learning for Mobile Robots</a></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/RL/nav-in-complex-env.html"><strong aria-hidden="true">2.3.5.</strong> LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS</a></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/RL/LRF/nav-dyn-env with DRL.html" class="active"><strong aria-hidden="true">2.3.6.</strong> Learning to Navigate Through Complex Dynamic Environment With Modular Deep Reinforcement Learning</a></li></ol></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        
                        <a href="../../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        

                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="learning-to-navigate-through-complex-dynamic-environment-with-modular-deep-reinforcement-learning"><a class="header" href="#learning-to-navigate-through-complex-dynamic-environment-with-modular-deep-reinforcement-learning">Learning to Navigate Through Complex Dynamic Environment With Modular Deep Reinforcement Learning</a></h1>
<p>Main contributions:</p>
<ul>
<li>A modular architecture for navigation problems is proposed to separately resolve local obstacle avoidance and global navigation, which enables <strong>modularized training</strong> and promotes <strong>generalization ability</strong>.</li>
<li>A novel two-stream Q-network is developed for obstacle avoidance. By <strong>separating spatial and temporal information</strong> from raw input and processing them individually, this new approach supplements temporal integration of agent observations and surpasses the conventional DQL approach in moving obstacle avoidance tasks.</li>
<li>We proposed an <strong>action scheduling method</strong>, which combines and exploits the pretrained policies for efficient exploration and online learning in unknown environments.</li>
</ul>
<h2 id="related-work"><a class="header" href="#related-work">RELATED WORK</a></h2>
<h3 id="traditional-robotics"><a class="header" href="#traditional-robotics">Traditional Robotics</a></h3>
<p>The Potential Field Method (PFM).</p>
<p>Vector field histogram (VFH), VFH+, VFH*.</p>
<p>Simultaneous localization and mapping (SLAM), vision-based SLAM.</p>
<h3 id="reinforcement-learning"><a class="header" href="#reinforcement-learning">Reinforcement Learning</a></h3>
<p>Hierarchical reinforcement learning.</p>
<p>The <em>option</em> framework, concurrent option framework, HAM, MAXQ.</p>
<p>DRL: Double Qlearning, Dueling network architecture, deep deterministic policy gradient method, A3C, Hierarchical-DQN.</p>
<h2 id="proposed-approaches"><a class="header" href="#proposed-approaches">PROPOSED APPROACHES</a></h2>
<p>Divide and conquer strategy, divide the main navigation task into two subtasks: <strong>local avoidance</strong> and <strong>global navigation</strong>.</p>
<h3 id="avoidance-module"><a class="header" href="#avoidance-module">Avoidance Module</a></h3>
<p>The simulated environment is formalized as a Markov Decision Process (MDP) described by a 4-tuple \(S, A, P, R\).</p>
<p>Time step \(t\), state \(s_t \in S\), action \(a_t \in A \), policy \(\pi\), reward \(r_t \sim R(s_t, a_t)\), new state \(s_{t+1} = P(s_t, a_t)\), return \(R_t = \sum_{t'=t}^T \gamma^{t'-t} r_t', \gamma \in [0, 1]\).</p>
<p>Action-value function or Q-function: \(Q^\pi(s, a) = \mathbb{E}[R_t | s_t=s, a_t=a]\).</p>
<p>Optimal Action-value function or optimal Q-function: \(Q^*(s, a) = \max_\pi \mathbb{E}[R_t | s_t=s, a_t=a]\).</p>
<p>This optimal action-value function verifies the <em>Bellman optimality equation</em>: \(Q^<em>(s, a) = \mathbb{E}[r + \gamma \max_{a'} Q^</em>(s', a') | s, a]\).</p>
<p>Then, the Q-function can be estimated by utilizing this equation as an iterative update, which is given by</p>
<p>$$
Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)].
$$</p>
<p>For this avoidance task with LRF observations in a dynamic environment, we proposed a <strong>two-stream Q-network</strong> to approximate the action-value function.</p>
<p><img src="assets/nav_dyn_env-1.png" alt="Fig 1." /></p>
<p>The spatial part is the raw laser scan \(o_t\); the temporal part is the difference between the present and the previous laser scans, namely, \(d_t = o_t - o_{t-1}\).</p>
<p>The two-stream Q-network is present as \(Q^\theta (s_t, a_t), s_t = (o_t, d_t)\).</p>
<p>Two main techniques are employed:</p>
<ul>
<li>experience replay
<ul>
<li>The agent’s transition experience \(e = (s_t, a_t, r_t, s_{t+1})\) of each step is stored in a replay buffer \(D = (e_1, \dots, e_N)\).</li>
</ul>
</li>
<li>double Q-learning
<ul>
<li>A separated network termed <strong>target network</strong> is introduced to estimate the target for gradient descent update of the main network.</li>
</ul>
</li>
</ul>
<p>The weights of the main network at training iteration i are updated by minimizing the loss function:</p>
<p>$$
L_i(\theta_i) = \mathbb{E}_{e \sim D} [(y_i - Q^{\theta_i}(s_t, a_t))^2]
$$</p>
<p>$$
y_i = r + \gamma \hat{Q}^{\theta^-}(s_{t+1}, \argmax_{a'} Q^{\theta_i}(s_{t+1}, a'))
$$</p>
<p>\(y_i\) is the updating target from the separated target network \(\hat{Q}\) with parameters\(\theta^-\).</p>
<p>The update is performed by gradient descent:</p>
<p>$$
\nabla_{\theta_i}L_i(\theta_i) = \mathbb{E}<em>{e \sim D} [(y_i - Q^{\theta_i}(s_t, a_t)) \nabla</em>{\theta_i} Q^{\theta_i}(s_t, a_t)]
$$</p>
<p>$$
\theta_{i+1} \leftarrow \theta_i - \alpha \nabla_{\theta_i}L_i(\theta_i)
$$</p>
<p>The learned avoidance control policy: \(\pi^\theta(s) = \argmax_a Q^\theta(s, a)\).</p>
<p>To ensure continual exploration, the action is selected by an \(\epsilon\)-greedy strategy.</p>
<h3 id="navigation-module"><a class="header" href="#navigation-module">Navigation Module</a></h3>
<p>Two parts:</p>
<ul>
<li>the offline part
<ul>
<li>solve the <strong>simple navigation subtask</strong>: seeking the fastest policy to drive the agent directly to the destination only based on the relative coordinates in a blank environment without any moving obstacles.</li>
<li>standard Q-learning algorithm: input is the 2-D vector \(p_t\), a large positive reward for reaching the destination and a small negative reward for every time step to urge the agent to get the destination in the fastest way.</li>
</ul>
</li>
<li>the online part
<ul>
<li>two inputs: 1) a 2-D relative coordinates vector of the destination \(p_t\); 2) a 128-D feature \(\phi\) from the avoidance module, which conveys both spatial and temporal information of the surrounding situation.</li>
<li>a positive reward for reaching the destination and a negative reward for collision and also a small time punishment in each step.</li>
<li>the training procedure follows the DQL algorithm, experience replay and double Q-learning.</li>
<li>an <strong>action scheduler</strong> to pick up the actions from three action alternatives.</li>
</ul>
</li>
</ul>
<h3 id="action-scheduler"><a class="header" href="#action-scheduler">Action Scheduler</a></h3>
<p>The real-time risk factor \(\eta\) is defined as</p>
<p>$$
\eta = \frac{1}{\min(o_t - \kappa d_t)}, \kappa \in [0, 1]
$$</p>
<p>switching threshold \(\lambda\), the switching strategy:</p>
<p>$$
a = \begin{cases}
action_{n2} \ \ \eta &lt; \lambda \
action_a \ \ \eta \geq \lambda \
\end{cases}
$$</p>
<p>where action_a and action_n2 are the action alternatives offered by the avoidance module and the offline part of the navigation module, respectively. </p>
<p>To guarantee a smooth transfer, we introduce a self-decaying transfer probability \(\varphi\). In each step, the agent follows the heuristic policy with the probability \(\varphi\) for heuristic exploration, with probability \((1-\varphi)(1-\epsilon)\) to exploit its online learning policy.</p>
<p>Besides, we still keep a small probability \(\epsilon\) for random exploration to further promote the performance. The (\varphi\) starts from 1 and decays in each step utill 0.</p>
<p><img src="assets/nav_dyn_env-2.png" alt="Fig 2." /></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="../../../autonomous-systems/RL/nav-in-complex-env.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="../../../autonomous-systems/RL/nav-in-complex-env.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="../../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="../../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>
