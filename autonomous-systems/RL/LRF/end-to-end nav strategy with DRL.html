<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>End-to-End Navigation Strategy With Deep Reinforcement Learning for Mobile Robots</title>
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="../../../favicon.svg">
        
        
        <link rel="shortcut icon" href="../../../favicon.png">
        
        <link rel="stylesheet" href="../../../css/variables.css">
        <link rel="stylesheet" href="../../../css/general.css">
        <link rel="stylesheet" href="../../../css/chrome.css">
        
        <link rel="stylesheet" href="../../../css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="../../../FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="../../../fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../../../highlight.css">
        <link rel="stylesheet" href="../../../tomorrow-night.css">
        <link rel="stylesheet" href="../../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../../../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../../../meta-learning/index.html"><strong aria-hidden="true">1.</strong> Meta-Learning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../meta-learning/MAML.html"><strong aria-hidden="true">1.1.</strong> Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a></li><li class="chapter-item expanded "><a href="../../../meta-learning/MAML++.html"><strong aria-hidden="true">1.2.</strong> HOW TO TRAIN YOUR MAML</a></li><li class="chapter-item expanded "><a href="../../../meta-learning/ANIL.html"><strong aria-hidden="true">1.3.</strong> RAPID LEARNING OR FEATURE REUSE? TOWARDS UNDERSTANDING THE EFFECTIVENESS OF MAML</a></li><li class="chapter-item expanded "><a href="../../../meta-learning/FO-MAML.html"><strong aria-hidden="true">1.4.</strong> On First-Order Meta-Learning Algorithms</a></li><li class="chapter-item expanded "><a href="../../../meta-learning/HF-MAML.html"><strong aria-hidden="true">1.5.</strong> On the Convergence Theory of Gradient-Based Model-Agnostic Meta-Learning Algorithms</a></li><li class="chapter-item expanded "><a href="../../../meta-learning/iMAML.html"><strong aria-hidden="true">1.6.</strong> Meta-Learning with Implicit Gradients</a></li></ol></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/index.html"><strong aria-hidden="true">2.</strong> Autonomous Systems</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../autonomous-systems/planning&decision-making.html"><strong aria-hidden="true">2.1.</strong> [review]Planning and Decision-Making for Autonomous Vehicles</a></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/motion-planning-review.html"><strong aria-hidden="true">2.2.</strong> [review]A Review of Mobile Robot Motion Planning Methods</a></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/RL/index.html"><strong aria-hidden="true">2.3.</strong> RL</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../../../autonomous-systems/RL/A3C.html"><strong aria-hidden="true">2.3.1.</strong> Asynchronous Methods for Deep Reinforcement Learning (A3C)</a></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/RL/nav-in-complex-env.html"><strong aria-hidden="true">2.3.2.</strong> LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS</a></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/RL/LRF/curiosity-driven.html"><strong aria-hidden="true">2.3.3.</strong> Curiosity-driven Exploration by Self-supervised Prediction</a></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/RL/LRF/end-to-end nav strategy with DRL.html" class="active"><strong aria-hidden="true">2.3.4.</strong> End-to-End Navigation Strategy With Deep Reinforcement Learning for Mobile Robots</a></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/RL/nav-in-complex-env.html"><strong aria-hidden="true">2.3.5.</strong> LEARNING TO NAVIGATE IN COMPLEX ENVIRONMENTS</a></li><li class="chapter-item expanded "><a href="../../../autonomous-systems/RL/LRF/nav-dyn-env with DRL.html"><strong aria-hidden="true">2.3.6.</strong> Learning to Navigate Through Complex Dynamic Environment With Modular Deep Reinforcement Learning</a></li></ol></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        
                        <a href="../../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        

                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="end-to-end-navigation-strategy-with-deep-reinforcement-learning-for-mobile-robots"><a class="header" href="#end-to-end-navigation-strategy-with-deep-reinforcement-learning-for-mobile-robots">End-to-End Navigation Strategy With Deep Reinforcement Learning for Mobile Robots</a></h1>
<h2 id="problem-statement"><a class="header" href="#problem-statement">Problem Statement</a></h2>
<p>Navigation strategies for mobile robots in a map-less environment.</p>
<p>A motion planning model based on DRL. This end-to-end model transfers directly the inputs of the sensor observations and the relative position of the target to the commands of robot’s movement.</p>
<p>Uses <em>intrinsic curiosity</em> to allow agents to explore the environment more effectively and as an additional reward in an environment where the rewards are sparse. <em>Sparse laser ranging results</em> are also used as the highly abstracted inputs, so agent strategies that are learned by the simulation are effective in the real world.</p>
<h2 id="curiosity-driven-exploration"><a class="header" href="#curiosity-driven-exploration">Curiosity-Driven Exploration</a></h2>
<p>The original states \((s_t,s_{t+1})\) are encoded into the corresponding features \((\phi(s_t), \phi(s_{t+1}))\).</p>
<p>The inverse model is trained to predict at using state features \((\phi(s_t),\phi(s_{t+1}))\). </p>
<p>\(a_t\) and \(\phi(s_t)\) are passed to the forward model, which is used to predict the feature representation \(\phi(s_{t+1})\) for the state \((s_{t+1})\) and compare it with the real feature \((\phi(s_t))\).</p>
<p>For the inverse model, the goal is to learn the function \(g\) as follows:</p>
<p>$$
\hat{a}<em>t = g(s_t, s</em>{t+1} ; \theta_I)
$$</p>
<p>where \(\hat{a}_t\) is the predicted value of the agent’s action \((a_t)\) and \(\theta_I\) is the neural network parameter that is trained to be optimized as follows:</p>
<p>$$
\min_{\theta_I} L_I(\hat{a}_t, a_t)
$$</p>
<p>Since the action space is discrete, the output of \(g\) is a soft-max distribution across all possible actions. </p>
<p>The loss function \(L_I\) is set in the form of cross entropy as follows:</p>
<p>$$
L_I(\hat{a}<em>t, a_t) = \sum</em>{i=1}^n -P(a_{t_i}) \ln q(\hat{a}_{t_i})
$$</p>
<p>where \(n\) is the size of the action space, \(P(a_{t_i})\) is whether the agent chooses the \(i\)th action in the actual situation, and \(q(\hat{a}_{t_i})\) is the probability of the agent choosing the \(i\)th action in the prediction result.</p>
<p>Another neural network is trained for the forward model:</p>
<p>$$
\hat{\phi}(s_{t+1}) = f(\phi(s_t), a_t, \theta_F)
$$</p>
<p>The training process is performed by minimizing the loss function LF as follows:</p>
<p>$$
L_F(\phi(s_t), \hat{\phi}(s_{t+1})) = \frac{1}{2} \parallel \hat{\phi}(s_{t+1}) - \phi(s_{t+1}) \parallel _2^2
$$</p>
<p>The intrinsic reward signal \(r_t^i\) measured by the prediction error of the feature encoding is calculated as:</p>
<p>$$
r_t^i = \frac{\eta}{2} \parallel \hat{\phi}(s_{t+1}) - \phi(s_{t+1}) \parallel _2^2
$$</p>
<p>where \(\eta &gt; 0\) is the scale factor used to adjust the intrinsic reward.</p>
<p><img src="assets/navDRL-1.png" alt="Fig 1." /></p>
<h2 id="navigation-strategy-based-on-drl"><a class="header" href="#navigation-strategy-based-on-drl">Navigation Strategy Based on DRL</a></h2>
<p><img src="assets/navDRL-2.png" alt="Fig 2." /></p>
<p>The first layer of LSTM accepts the previous rewards and observations. The aim is to establish an association between the original observations and rewards and transmit this to the next level of LSTM. The previous action is passed directly to the LSTM of the second layer.</p>
<p>Finally, connected to two separate output layers including softmax layer and linear layer, respectively, generating policy \(\pi\) and value function \(V\).</p>
<p>The reward function is divided into two components: the agent interacting with the environment obtains the extrinsic reward \(r_t^e\) and the intrinsic reward \(r_t^i\).</p>
<p>The policy \(\pi(s_t ; \theta_p)\) is represented by a deep neural network with the parameter \(\theta_p\).</p>
<p>Sum of the intrinsic reward: \(r_t = r_t^e + r_t^i\).</p>
<p>The problem that needs to be optimized overall:</p>
<p>$$
\min_{\theta_p, \theta_I, \theta_F} \Big [-\lambda \mathbb{E}_{\pi(s_t ; \theta_p)}\big [ \sum_t r_t \big ] + (1-\beta) L_I + \beta L_F \Big ]
$$</p>
<p>where \(\lambda &gt; 0\) is used to adjust the weight of the policy gradient loss for the curiosity reward, and \(0 \leq \beta \leq 1\) is used to adjust the weight between the forward model loss and the inverse model loss.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="../../../autonomous-systems/RL/LRF/curiosity-driven.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="../../../autonomous-systems/RL/nav-in-complex-env.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a rel="prev" href="../../../autonomous-systems/RL/LRF/curiosity-driven.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="../../../autonomous-systems/RL/nav-in-complex-env.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="../../../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="../../../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../../../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        

    </body>
</html>
